import argparse
from typing import Any, List, Optional

from dataflow.utils.storage import FileStorage
from dataflow.serving.local_model_vlm_serving import LocalModelVLMServing_vllm

# ğŸ‘‡ æ”¹æˆä½ ä¿å­˜ç®—å­çš„æ–‡ä»¶å
# from operator_scalecap_single import (
#     ScaleCapCaptionBuildSingle,
#     ScaleCapSingleConfig,
# )
from dataflow.operators.core_vision import ImageScaleCaptionGenerate, ImageScaleCaptionGenerateConfig


class ScaleCapPipeline:
    """
    ä¸€è¡Œå‘½ä»¤å³å¯æŠŠ {"image": "..."} jsonl å¤„ç†æˆ ScaleCap é£æ ¼çš„é•¿æè¿°æ•°æ®ï¼š
        åˆç¨¿ â†’ å¥çº§å¯¹æ¯”æ‰“åˆ†ï¼ˆgoldensï¼‰â†’ å¯¹è±¡/ä½ç½®è¿½é—® â†’ å›ç­”ï¼ˆå¯é€‰äºŒæ¬¡è¿‡æ»¤ï¼‰â†’ final_caption
    """

    def __init__(
        self,
        vlm_model_path: str,
        *,
        llm_model_path: Optional[str] = None,   # å¯é€‰ï¼šä¸ä¼ åˆ™ä¸ VLM å…±ç”¨åŒä¸€æœåŠ¡
        hf_cache_dir: Optional[str] = None,
        download_dir: str = "./ckpt/models",
        device: str = "cuda",

        # Storage
        first_entry_file: str = "/path/to/your_images.jsonl",
        cache_path: str = "./cache_local",
        file_name_prefix: str = "scalecap",
        cache_type: str = "jsonl",

        # Keys
        image_key: str = "image",
        output_key: str = "scalecap_record",

        # ç®—å­é…ç½®
        tau_sentence: float = 0.15,
        max_questions: int = 20,
        max_init_tokens: int = 1024,
        max_answer_tokens: int = 256,
        second_filter: bool = False,

        # vLLM ç”Ÿæˆé…ç½®ï¼ˆæŒ‰éœ€æš´éœ²æ›´å¤šï¼‰
        vllm_tensor_parallel_size: int = 1,
        vllm_temperature: float = 0.0,
        vllm_top_p: float = 0.9,
        vllm_max_tokens: int = 512,
    ):
        # ---------- 1. Storage ----------
        self.storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type=cache_type,
        )

        # ---------- 2. Serving ----------
        # VLMï¼ˆå¿…é¡»ï¼‰
        self.vlm_serving = LocalModelVLMServing_vllm(
            hf_model_name_or_path=vlm_model_path,
            hf_cache_dir=hf_cache_dir,
            hf_local_dir=download_dir,
            vllm_tensor_parallel_size=vllm_tensor_parallel_size,
            vllm_temperature=vllm_temperature,
            vllm_top_p=vllm_top_p,
            vllm_max_tokens=vllm_max_tokens,
        )
        # LLMï¼ˆå¯é€‰ï¼›ä¸ä¼ åˆ™ç”¨ VLM æœåŠ¡åšæ•´åˆï¼‰
        self.llm_serving = None
        if llm_model_path:
            self.llm_serving = LocalModelVLMServing_vllm(
                hf_model_name_or_path=llm_model_path,
                hf_cache_dir=hf_cache_dir,
                hf_local_dir=download_dir,
                vllm_tensor_parallel_size=vllm_tensor_parallel_size,
                vllm_temperature=0.0,     # æ•´åˆå»ºè®®ç”¨ç¡®å®šæ€§
                vllm_top_p=1.0,
                vllm_max_tokens=2048,
            )

        # ---------- 3. Operator ----------
        cfg = ImageScaleCaptionGenerateConfig(
            tau_sentence=tau_sentence,
            max_questions=max_questions,
            max_init_tokens=max_init_tokens,
            max_answer_tokens=max_answer_tokens,
            second_filter=second_filter,
            input_jsonl_path=None,     # ç”¨ storage é©±åŠ¨ï¼Œæ— éœ€ç›´ä¼ 
            output_jsonl_path=None
        )

        self.operator = ImageScaleCaptionGenerate(
            vlm_serving=self.vlm_serving,
            config=cfg,
        )

        # è®°å½•å­—æ®µ
        self.image_key = image_key
        self.output_key = output_key

    # ------------------------------------------------------------------ #
    def forward(self):
        """
        ä¸€é”®è·‘å®Œå½“å‰ pipeline ä¸€ä¸ª stepï¼š
            å›¾ç‰‡ â†’ ï¼ˆinit_caption â†’ goldens â†’ QAs â†’ final_captionï¼‰â†’ å†™å› storage
        """
        self.operator.run(
            storage=self.storage.step(),   # Pipeline è´Ÿè´£æ¨è¿› step
            image_key=self.image_key,
            output_key=self.output_key,
        )
        print("[ScaleCapPipeline] Done. Cached to:", self.storage.cache_path)


# ---------------------------- CLI å…¥å£ -------------------------------- #
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ScaleCap Dense Captioning Pipeline (DataFlow)")

    # æ¨¡å‹
    parser.add_argument("--vlm_model_path", default="/mnt/public/model/huggingface/Qwen2.5-VL-3B-Instruct")
    parser.add_argument("--llm_model_path", default="")  # å¯ä¸å¡«ï¼šé»˜è®¤ä¸ VLM å…±ç”¨
    parser.add_argument("--hf_cache_dir", default="~/.cache/huggingface")
    parser.add_argument("--download_dir", default="./ckpt/models")
    parser.add_argument("--device", choices=["cuda", "cpu", "mps"], default="cuda")

    # è¾“å…¥/ç¼“å­˜
    parser.add_argument("--images_file", default="./dataflow/example/image_to_text_pipeline/scale_cap_captions.jsonl")
    parser.add_argument("--cache_path", default="./cache_local")
    parser.add_argument("--file_name_prefix", default="scalecap")
    parser.add_argument("--cache_type", default="jsonl")

    # å­—æ®µå
    parser.add_argument("--image_key", default="image")
    parser.add_argument("--output_key", default="scalecap_record")

    # ç®—å­é…ç½®
    parser.add_argument("--tau_sentence", type=float, default=0.15)
    parser.add_argument("--max_questions", type=int, default=20)
    parser.add_argument("--max_init_tokens", type=int, default=1024)
    parser.add_argument("--max_answer_tokens", type=int, default=256)
    parser.add_argument("--second_filter", action="store_true")

    # vLLM ç”Ÿæˆé…ç½®ï¼ˆæŒ‰éœ€æ‰©å±•ï¼‰
    parser.add_argument("--tp", type=int, default=1)
    parser.add_argument("--temperature", type=float, default=0.0)
    parser.add_argument("--top_p", type=float, default=0.9)
    parser.add_argument("--max_tokens", type=int, default=512)

    args = parser.parse_args()

    pipe = ScaleCapPipeline(
        vlm_model_path=args.vlm_model_path,
        llm_model_path=(args.llm_model_path or None),
        hf_cache_dir=args.hf_cache_dir,
        download_dir=args.download_dir,
        device=args.device,

        first_entry_file=args.images_file,
        cache_path=args.cache_path,
        file_name_prefix=args.file_name_prefix,
        cache_type=args.cache_type,

        image_key=args.image_key,
        output_key=args.output_key,

        tau_sentence=args.tau_sentence,
        max_questions=args.max_questions,
        max_init_tokens=args.max_init_tokens,
        max_answer_tokens=args.max_answer_tokens,
        second_filter=args.second_filter,

        vllm_tensor_parallel_size=args.tp,
        vllm_temperature=args.temperature,
        vllm_top_p=args.top_p,
        vllm_max_tokens=args.max_tokens,
    )
    pipe.forward()
