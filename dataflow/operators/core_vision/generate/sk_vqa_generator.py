from dataflow.core.Operator import OperatorABC

from dataflow.prompts.image import SKVQAGeneratorPrompt
import pandas as pd
from dataflow.utils.registry import OPERATOR_REGISTRY
from dataflow import get_logger

from dataflow.utils.storage import FileStorage, DataFlowStorage
from dataflow.core import LLMServingABC
from dataflow.serving.local_model_vlm_serving import LocalModelVLMServing_vllm

from qwen_vl_utils import process_vision_info

import re

def normalize_whitespace(s: str) -> str:
    """Collapse whitespace to single spaces and trim."""
    return re.sub(r'\s+', ' ', s or '').strip()

def parse_wiki_qa(text: str) -> dict:
    """
    è§£æåŒ…å« '### Wikipedia Article' å’Œ '### Question Answer Pairs' çš„æ–‡æœ¬ã€‚
    è¿”å›æ ¼å¼:
    {
        "context": "æ–‡ç« å†…å®¹",
        "qas": [
            {"question": "é—®é¢˜", "answer": "ç­”æ¡ˆ"},
            ...
        ]
    }
    """
    if not isinstance(text, str) or not text.strip():
        return {"context": "", "qas": []}

    try:
        # å»é™¤å¤šä½™çš„æ˜Ÿå·ã€ç©ºè¡Œ
        text_clean = re.sub(r'(?<!\*)\*(?!\*)', '', text)
        text_clean = text_clean.strip()

        # æå– Wikipedia Article æ®µè½
        m_article = re.search(
            r'###\s*Wikipedia Article\s*(.*?)\n###\s*Question Answer Pairs',
            text_clean, flags=re.DOTALL | re.IGNORECASE
        )
        article = normalize_whitespace(m_article.group(1)) if m_article else ""

        # æå– QA æ®µè½
        qa_section_match = re.search(
            r'###\s*Question Answer Pairs\s*(.*)',
            text_clean, flags=re.DOTALL | re.IGNORECASE
        )
        qas = []

        if qa_section_match:
            qa_section = qa_section_match.group(1).strip()

            # ä¼˜å…ˆç”¨æ­£åˆ™æ‰¹é‡åŒ¹é…é—®ç­”å¯¹
            pattern = re.compile(
                r'\d+\.\s*\*\*(.*?)\*\*\s*(?:\r?\n|\s)*-+\s*(.+?)(?=(?:\n\d+\.|\Z))',
                flags=re.DOTALL
            )
            matches = pattern.findall(qa_section)

            for q, a in matches:
                q_text = normalize_whitespace(q)
                a_text = normalize_whitespace(a.replace('\n', ' '))
                a_text = re.sub(r'^\-+\s*', '', a_text).replace('*', '')
                if q_text and a_text:
                    qas.append({"question": q_text, "answer": a_text})

            # å¦‚æœæ²¡åŒ¹é…åˆ°ï¼Œç”¨ç®€å•è¡Œçº§åŒ¹é…å®¹é”™
            if not qas:
                lines = qa_section.splitlines()
                cur_q = None
                for line in lines:
                    line = line.strip()
                    if re.match(r'^\d+\.\s*\*\*(.+)\*\*$', line):
                        cur_q = re.sub(r'^\d+\.\s*\*\*(.+)\*\*$', r'\1', line).strip()
                    elif line.startswith('-') and cur_q:
                        ans = line.lstrip('-').strip()
                        if cur_q and ans:
                            qas.append({
                                "question": normalize_whitespace(cur_q),
                                "answer": normalize_whitespace(ans)
                            })
                        cur_q = None

        # æœ€ç»ˆç»“æœ
        return {"context": article, "qas": qas}

    except Exception as e:
        # ä»»æ„å¼‚å¸¸æ—¶å®‰å…¨è¿”å›ç©ºç»“æ„
        return {"context": "", "qas": []}


@OPERATOR_REGISTRY.register()
class ImageSKVQAGenerate(OperatorABC):
    '''
    SKVQA Generator is a class that generates structured visual questionâ€“answer descriptions for given images.
    '''
    def __init__(self, llm_serving: LLMServingABC):
        self.logger = get_logger()
        self.prompt_generator = SKVQAGeneratorPrompt()
        self.llm_serving = llm_serving

    @staticmethod
    def get_desc(lang: str = "zh"):
        if lang == "zh":
            return (
                "è¯¥ç®—å­ç”¨äºç”Ÿæˆ Synthetic Knowledge VQAï¼ˆSKVQAï¼‰ç»“æœã€‚\n\n"
                "ğŸ“˜ ä»€ä¹ˆæ˜¯ SKVQAï¼š\n"
                "  - SKVQAï¼ˆåˆæˆçŸ¥è¯†è§†è§‰é—®ç­”ï¼‰æ˜¯åœ¨æ™®é€š VQA çš„åŸºç¡€ä¸Šå¢åŠ äº†â€œä¸Šä¸‹æ–‡ (context)â€ä¿¡æ¯ï¼Œ\n"
                "    æ¨¡å‹ä¸ä»…æ ¹æ®å›¾åƒå†…å®¹å›ç­”é—®é¢˜ï¼Œè¿˜éœ€ç»“åˆç»™å®šçš„èƒŒæ™¯çŸ¥è¯†æˆ–æ–‡æœ¬ç‰‡æ®µè¿›è¡Œæ¨ç†ã€‚\n"
                "  - è¿™æ ·å¯ä»¥è®©æ¨¡å‹åœ¨é¢å¯¹å¤æ‚æˆ–çŸ¥è¯†ç›¸å…³çš„é—®é¢˜æ—¶ï¼Œæ›´å¥½åœ°ç†è§£åœºæ™¯å¹¶ç”Ÿæˆåˆç†ç­”æ¡ˆã€‚\n\n"
                "ğŸ§© åŠŸèƒ½è¯´æ˜ï¼š\n"
                "  - è¾“å…¥å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰åï¼Œè‡ªåŠ¨æ„é€ æç¤ºè¯å¹¶è°ƒç”¨è§†è§‰è¯­è¨€å¤§æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–é—®ç­”è¾“å‡ºã€‚\n"
                "  - è¾“å‡ºæ ¼å¼ä¸ºï¼š\n"
                "    {\n"
                "      'context': 'ä¸å›¾åƒç›¸å…³çš„ä¸Šä¸‹æ–‡',\n"
                "      'qas': [\n"
                "        {'question': 'é—®é¢˜1', 'answer': 'ç­”æ¡ˆ1'},\n"
                "        {'question': 'é—®é¢˜2', 'answer': 'ç­”æ¡ˆ2'}\n"
                "      ]\n"
                "    }\n\n"
                "ğŸ§  ä¸æ™®é€š VQA çš„åŒºåˆ«ï¼š\n"
                "  - æ™®é€š VQAï¼šä»…æ ¹æ®å›¾åƒæœ¬èº«å›ç­”ã€‚\n"
                "  - SKVQAï¼šåœ¨å›ç­”æ—¶ç»“åˆä¸Šä¸‹æ–‡å†…å®¹ï¼Œæ›´è´´è¿‘çœŸå®æ¨ç†ä¸çŸ¥è¯†ç†è§£ã€‚\n\n"
                "âš™ï¸ å‚æ•°è¯´æ˜ï¼š\n"
                "  - multi_modal_key: è¾“å…¥å›¾åƒæ‰€åœ¨åˆ—åï¼Œé»˜è®¤ 'image'ã€‚\n"
                "  - output_key: è¾“å‡ºç»“æœåˆ—åï¼Œé»˜è®¤ 'skvqa'ã€‚\n\n"
                "ğŸ’¡ å…¸å‹åº”ç”¨åœºæ™¯ï¼š\n"
                "  - å›¾åƒ + äº§å“è¯´æ˜ â†’ è‡ªåŠ¨ç”Ÿæˆäº§å“é—®ç­”ã€‚\n"
                "  - å›¾ç‰‡ + æ–‡æ¡£å†…å®¹ â†’ è§†è§‰çŸ¥è¯†ç†è§£ã€‚\n"
                "  - å¤šæ¨¡æ€çŸ¥è¯†èåˆè®­ç»ƒæˆ–æ•°æ®å¢å¼ºã€‚"
            )
        else:
            return (
                "This operator generates Synthetic Knowledge VQA (SKVQA) outputs.\n\n"
                "ğŸ“˜ What is SKVQA:\n"
                "  - SKVQA (Synthetic Knowledge Visual Question Answering) extends normal VQA by adding a textual 'context'.\n"
                "  - The model answers questions not only from the image but also by reasoning with the provided background text.\n\n"
                "ğŸ§© Function:\n"
                "  - Takes images as input, builds prompts automatically, and uses a vision-language model to generate structured Q&A.\n"
                "  - Output format:\n"
                "    {\n"
                "      'context': 'related background information',\n"
                "      'qas': [\n"
                "        {'question': 'Question 1', 'answer': 'Answer 1'},\n"
                "        {'question': 'Question 2', 'answer': 'Answer 2'}\n"
                "      ]\n"
                "    }\n\n"
                "ğŸ§  Difference from normal VQA:\n"
                "  - Normal VQA: answers purely from the image.\n"
                "  - SKVQA: answers by combining visual evidence with external knowledge or text context.\n\n"
                "âš™ï¸ Parameters:\n"
                "  - multi_modal_key: name of the image column (default 'image').\n"
                "  - output_key: name of the output column (default 'skvqa').\n\n"
                "ğŸ’¡ Typical use cases:\n"
                "  - Image + product description â†’ auto-generate product Q&A.\n"
                "  - Image + document â†’ visual knowledge reasoning.\n"
                "  - Multimodal data augmentation or reasoning tasks."
            )

    
    def _validate_dataframe(self, dataframe: pd.DataFrame):
        required_keys = [self.multi_modal_key]
        forbidden_keys = [self.output_key]

        missing = [k for k in required_keys if k not in dataframe.columns]
        conflict = [k for k in forbidden_keys if k in dataframe.columns]

        if missing:
            raise ValueError(f"Missing required column(s): {missing}")
        if conflict:
            raise ValueError(f"The following column(s) already exist and would be overwritten: {conflict}")

    def _prepare_batch_inputs(self, media_paths):
        """
        Construct batched prompts and image inputs from media paths.
        """
        prompts = self.prompt_generator.build_prompt()

        prompt_list = []
        image_inputs_list = []

        for paths in media_paths:
            for p in paths:
                raw_prompt = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": p},
                            {"type": "text", "text": prompts},
                        ],
                    },
                ]
                # Get vision inputs
                image_inputs, _ = process_vision_info(raw_prompt)

                # Format prompt using LLM processor
                prompt = self.llm_serving.processor.apply_chat_template(
                    raw_prompt, tokenize=False, add_generation_prompt=True
                )

                image_inputs_list.append(image_inputs)
                prompt_list.append(prompt)

        return prompt_list, image_inputs_list

    def run(
        self,
        storage: DataFlowStorage,
        input_modal_key: str = "image", 
        output_key: str = "skvqa"
    ):
        """
        Runs the SKVQA generation process in batch mode, reading from the input file and saving results to output.
        """
        self.multi_modal_key, self.output_key = input_modal_key, output_key
        dataframe = storage.read("dataframe")
        self._validate_dataframe(dataframe)
        
        media_paths = dataframe.get(self.multi_modal_key, pd.Series([])).tolist()
        media_paths = [path if isinstance(path, list) else [path] for path in media_paths]
        
        prompt_list, image_inputs_list = self._prepare_batch_inputs(media_paths)

        outputs = self.llm_serving.generate_from_input(
            user_inputs=prompt_list,
            image_inputs=image_inputs_list
        )

        # æå–contextå’Œqaï¼Œç„¶åå­˜åˆ°skvqaè¿™ä¸ªkeyä¸‹é¢
        # æ‰¹é‡è§£ææ¯ä¸ªè¾“å‡º
        skvqa_results = []
        for out in outputs:
            skvqa_results.append(parse_wiki_qa(out))
        dataframe[self.output_key] = skvqa_results

        output_file = storage.write(dataframe)
        self.logger.info(f"Results saved to {output_file}")

        return [output_key]


if __name__ == "__main__":
    # Initialize model
    model = LocalModelVLMServing_vllm(
        hf_model_name_or_path="Qwen/Qwen2.5-VL-3B-Instruct",
        vllm_tensor_parallel_size=1,
        vllm_temperature=0.7,
        vllm_top_p=0.9,
        vllm_max_tokens=512,
    )

    skvqa_generator = ImageSKVQAGenerate(
        llm_serving=model
    )

    # Prepare input
    storage = FileStorage(
        first_entry_file_name="dataflow/example/image_to_text_pipeline/capsbench_captions.jsonl", 
        cache_type="jsonl"
    )
    storage.step()  # Load the data

    skvqa_generator.run(
        storage=storage,
        input_modal_key="image",
        output_key="skvqa"
    )